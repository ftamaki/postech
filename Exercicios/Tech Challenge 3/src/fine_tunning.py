# fine_tunning.py
# Resumo do script
#   -Esse script realiza o fine-tuning de um modelo de linguagem usando LoRA e TRL.
# Fine tunning significa ajustar um modelo pr√©-treinado em um conjunto de dados espec√≠fico para melhorar seu desempenho em uma tarefa particular.
#   - Nesse caso, o script utiliza um dataset JSONL para treinar o modelo o fine_tunning_dataset.jsonl localizado na pasta data.
#   - Fine tuning √© uma t√©cnica comum em aprendizado de m√°quina, especialmente em modelos de linguagem natural.
# Este script realiza o fine-tuning de um modelo de linguagem usando LoRA (Low-Rank Adaptation) e TRL (Transformers Reinforcement Learning).
# Requer bibliotecas: transformers, peft, trl, datasets, torch.
# Configura√ß√µes de modelo, LoRA e argumentos de treinamento s√£o definidos.
# O script carrega o dataset, inicializa o modelo e o treinador, e executa o fine-tuning.
import os
from pathlib import Path                    # Manipula√ß√£o de caminhos de arquivos
from datasets import load_dataset, Dataset  # Carregamento e manipula√ß√£o de datasets
from transformers import (                  
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)                                           # AutoModelForCausalLM: Carrega modelos de linguagem causal pr√©-treinados
                                            # AutoTokenizer: Tokenizador para modelos de linguagem
                                            # BitsAndBytesConfig: Configura√ß√£o para quantiza√ß√£o de modelos
                                            # TrainingArguments: Argumentos para treinamento de modelos
from peft import LoraConfig                 # Configura√ß√£o do LoRA para fine-tuning eficiente, Low-Rank Adaptation 
from trl import SFTTrainer                  # SFTTrainer: Treinador especializado para fine-tuning supervisionado de modelos de linguagem
import torch                                # Biblioteca principal para computa√ß√£o em tensores e aprendizado de m√°quina
# atrav√©s do torch que realizamos opera√ß√µes em GPU/CPU, manipula√ß√£o de tensores, constru√ß√£o e treinamento de modelos neurais
# nesse script usamos torch para definir o dtype (float16) e device_map (auto) ao carregar o modelo, permitindo otimiza√ß√£o de desempenho durante o fine-tuning
import json                                 # Manipula√ß√£o de dados JSON   
import traceback                            # Tratamento e exibi√ß√£o de rastreamentos de erros


# Script dividido em 5 partes principais:
# 1. Configura√ß√µes - Caminhos e nomes de modelos
# 2. Configura√ß√£o do LoRA - Par√¢metros do LoRA para fine-tuning
# 3. Argumentos de Treinamento - Configura√ß√µes para o processo de treinamento
# 4. Fun√ß√£o de Fine-Tuning - Carrega dados, modelo e executa o fine-tuning
# 5. Main + Debug - Execu√ß√£o do fine-tuning com tratamento de erros
# ==============================================================================
# 1. CONFIGURA√á√ïES
# ==============================================================================

PROJECT_ROOT = Path(__file__).parent.parent.resolve()               # Raiz do projeto (dois n√≠veis acima do arquivo atual)
DATASET_PATH = PROJECT_ROOT / "data" / "fine_tuning_dataset.jsonl"  # Caminho para o dataset de fine-tuning
OUTPUT_DIR = PROJECT_ROOT / "fine_tuned_model"                      # Diret√≥rio para salvar o modelo fine-tuned

model_name = "facebook/opt-125m"    # Modelo base pr√©-treinado, pode ser alterado para outro modelo compat√≠vel, escolhido por ser leve para testes
new_model = "med-assistant-lora"    # Nome do novo modelo fine-tuned que ser√° salvo


# ==============================================================================
# 2. LORA
# Low-Rank Adaptation (LoRA) √© uma t√©cnica de fine-tuning eficiente que adapta modelos pr√©-treinados
# reduzindo o n√∫mero de par√¢metros trein√°veis, permitindo ajustes r√°pidos com menos dados computacionais.
# LoRA insere matrizes de baixa-rank em camadas espec√≠ficas do modelo, facilitando o aprendizado de novas tarefas sem modificar os pesos originais do modelo.
# ==============================================================================

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"],
)   # Configura√ß√£o do LoRA para fine-tuning eficiente
    # r: rank das matrizes LoRA - determina a capacidade de adapta√ß√£o do modelo
    # lora_alpha: fator de escala para LoRA - controla a influ√™ncia das adapta√ß√µes LoRA
    # lora_dropout: taxa de dropout para regulariza√ß√£o - ajuda a prevenir overfitting
    # bias: tratamento de bias (nenhum nesse caso) - como lidar com termos de bias nas camadas adaptadas
    # task_type: tipo de tarefa (modelos de linguagem causal) - especifica a natureza da tarefa de fine-tuning
    # target_modules: m√≥dulos do modelo onde LoRA ser√° aplicado (proje√ß√µes Q e V) - define quais partes do modelo ser√£o adaptadas usando LoRA


# ==============================================================================
# 3. TRAINING ARGUMENTS
# Configura√ß√µes para o processo de treinamento do modelo
# ==============================================================================

training_arguments = TrainingArguments(
    output_dir=str(OUTPUT_DIR),
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=50,
    fp16=False,
    bf16=False,
    optim="adamw_torch",
)   # Argumentos de treinamento para o fine-tuning
    # output_dir: diret√≥rio para salvar checkpoints e o modelo final
    # num_train_epochs: n√∫mero de √©pocas de treinamento - quantas vezes o modelo ver√° todo o dataset durante o treinamento
    # per_device_train_batch_size: tamanho do batch por dispositivo (GPU/CPU) - quantos exemplos s√£o processados antes de atualizar os pesos do modelo
    # gradient_accumulation_steps: passos de acumula√ß√£o de gradiente para simular batches maiores - permite efetivamente aumentar o tamanho do batch sem aumentar o uso de mem√≥ria
    # learning_rate: taxa de aprendizado para o otimizador - controla a velocidade com que o modelo ajusta seus pesos durante o treinamento
    # logging_steps: frequ√™ncia de logging durante o treinamento - quantas vezes os logs de treinamento s√£o registrados
    # save_steps: frequ√™ncia de salvamento do modelo durante o treinamento - quantas vezes o modelo √© salvo durante o treinamento
    # fp16: se usar precis√£o de ponto flutuante 16 (n√£o usado)
    # bf16: se usar bfloat16 (n√£o usado)
    # optim: otimizador a ser usado (AdamW implementado no PyTorch) - especifica o algoritmo de otimiza√ß√£o para atualizar os pesos do modelo durante o treinamento


# ==============================================================================
# 4. FINE-TUNING
# Fun√ß√£o principal que realiza o fine-tuning do modelo
# ==============================================================================

def fine_tune_model():

    print(f"\nüìå Carregando dataset de: {DATASET_PATH}") # Carrega o dataset JSONL para fine-tuning

    try:
        data_list = []
        with open(DATASET_PATH, "r", encoding="utf-8") as f:
            for line in f:
                data_list.append(json.loads(line))      # Carrega manualmente o dataset JSONL linha por linha

        dataset = Dataset.from_list(data_list)          # Converte a lista de dicion√°rios em um Dataset do Hugging Face, cada linha do arquivo JSONL √© um exemplo separado no dataset      

    except Exception as e:
        print(f"Falhou ao carregar manualmente. Usando m√©todo HF. Erro: {e}")
        dataset = load_dataset("json", data_files=str(DATASET_PATH), split="train")

    if len(dataset) == 0:
        raise Exception("Dataset vazio! Verifique o arquivo JSONL.")

    print(f"‚úî Dataset carregado com {len(dataset)} exemplos")

    print(f"\nüìå Carregando modelo base: {model_name}")

    # Configura√ß√£o para quantiza√ß√£o do modelo usando 8 bits, reduzindo o uso de mem√≥ria

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        use_safetensors=True,   
    )   # Carrega o modelo pr√©-treinado com configura√ß√£o para uso em GPU/CPU e dtype float16
    # aqui usamos torch.float16 para reduzir o uso de mem√≥ria e acelerar o treinamento, especialmente em GPUs compat√≠veis

    # tokenizer utiliza AutoTokenizer para garantir compatibilidade com o modelo pr√©-treinado, aqui ele √© carregado com trust_remote_code=True para permitir o uso de c√≥digo personalizado hospedado remotamente, se necess√°rio
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) 
    # Carrega o tokenizador correspondente ao modelo pr√©-treinado,
    tokenizer.pad_token = tokenizer.eos_token # Define o token de padding como o token de fim de sequ√™ncia (eos_token)
    # Isso √© importante para garantir que o tokenizador possa lidar corretamente com sequ√™ncias de diferentes comprimentos durante o treinamento e infer√™ncia

    print("‚úî Modelo carregado")

    # ==============================================================================
    # TRL 0.25.1 SFT TRAINER - Fine-Tuning Supervisionado
    # Usamos o SFTTrainer do TRL para realizar o fine-tuning supervisionado do modelo
    # ==============================================================================

    print("\nüìå Inicializando SFTTrainer...")

    # Inicializa o treinador SFT com o modelo, argumentos de treinamento, dataset e configura√ß√£o do LoRA
    trainer = SFTTrainer(
        model=model,
        args=training_arguments,
        train_dataset=dataset,
        eval_dataset=None,
        peft_config=peft_config,
    )   
    # STFTrainer √© especializado para fine-tuning supervisionado de modelos de linguagem, facilitando a integra√ß√£o com LoRA e gerenciamento de datasets
    # model: modelo pr√©-treinado a ser fine-tuned
    # args: argumentos de treinamento definidos anteriormente
    # train_dataset: dataset de treinamento carregado
    # eval_dataset: dataset de avalia√ß√£o (nenhum nesse caso)
    # peft_config: configura√ß√£o do LoRA para adapta√ß√£o eficiente do modelo
    # essa configura√ß√£o permite que o treinador gerencie o processo de fine-tuning de forma eficiente, aplicando as adapta√ß√µes LoRA conforme especificado
    # isso simplifica o processo de treinamento, especialmente para grandes modelos de linguagem
 

    print("‚úî Trainer inicializado")

    # ==============================================================================
    # TREINAMENTO
    # Executa o fine-tuning do modelo usando o treinador configurado
    # execu√ß√£o simples mas eficaz do processo de treinamento
    # ==============================================================================

    print("\nüöÄ Iniciando treinamento...")
    trainer.train() # Inicia o processo de fine-tuning do modelo
    print("‚úî Treinamento conclu√≠do")

    print("\nüíæ Salvando modelo fine-tuned...")
    save_path = OUTPUT_DIR / new_model
    trainer.model.save_pretrained(str(save_path))
    tokenizer.save_pretrained(str(save_path))

    print(f"‚úî Modelo salvo em: {save_path}")


# ==============================================================================
# 5. MAIN + DEBUG
# Ponto de entrada do script com tratamento de erros para facilitar o debug
# ==============================================================================

if __name__ == "__main__": # Ponto de entrada do script __main__ indica que o c√≥digo dentro desse bloco ser√° executado apenas quando o script for executado diretamente, n√£o quando importado como m√≥dulo

    print("\n====================================================")
    print("üì¶ VERS√ïES DO AMBIENTE")
    print("====================================================")
    print(f"torch: {torch.__version__}")                                # Vers√£o do PyTorch, biblioteca principal para computa√ß√£o em tensores e aprendizado de m√°quina
    import transformers, trl, peft, accelerate, bitsandbytes, datasets  # Importa bibliotecas para exibir suas vers√µes
    print(f"transformers: {transformers.__version__}")                  # Vers√£o da biblioteca Transformers, usada para modelos de linguagem pr√©-treinados  
    print(f"trl: {trl.__version__}")                                    # Vers√£o da biblioteca TRL (Transformers Reinforcement Learning), usada para fine-tuning de modelos de linguagem
    print(f"peft: {peft.__version__}")                                  # Vers√£o da biblioteca PEFT (Parameter-Efficient Fine-Tuning), usada para fine-tuning eficiente com LoRA
    print(f"accelerate: {accelerate.__version__}")                      # Vers√£o da biblioteca Accelerate, usada para facilitar o treinamento em m√∫ltiplos dispositivos
    print(f"bitsandbytes: {bitsandbytes.__version__}")                  # Vers√£o da biblioteca BitsAndBytes, usada para quantiza√ß√£o de modelos
    print(f"datasets: {datasets.__version__}")                          # Vers√£o da biblioteca Datasets, usada para carregamento e manipula√ß√£o de datasets
    print("====================================================\n")

    # GPU CHECK
    print("üîç STATUS GPU:")
    if torch.cuda.is_available():                                        # Verifica se uma GPU CUDA est√° dispon√≠vel para uso
        print(f" - GPU detectada: {torch.cuda.get_device_name(0)}")      # Exibe o nome da GPU detectada
    else:
        print(" - Sem GPU CUDA detectada (treino ser√° no CPU ‚Äî lento!)") # Aviso se nenhuma GPU for detectada, indicando que o treinamento ser√° realizado na CPU, o que √© mais lento

    print("\n====================================================")
    print("üèÅ EXECUTANDO FINE-TUNING")
    print("====================================================")

    try:
        fine_tune_model()   # Chama a fun√ß√£o principal de fine-tuning, nesse caso dentro de um bloco try-except para capturar erros
        print("\n‚úÖ FINE-TUNING CONCLU√çDO COM SUCESSO")

    except Exception as e:
        print("\n‚ùå ERRO NO TREINAMENTO")
        print("Mensagem do erro:", str(e))
        print("\nüîç Traceback completo:")
        print(traceback.format_exc())
        print("\nüîß POSS√çVEIS CAUSAS:")
        print("1. Vers√£o incompat√≠vel do TRL, Transformers ou PEFT.")
        print("2. Campo 'text' ausente no JSONL.")
        print("3. Modelo muito grande para CPU.")
        print("4. BitsAndBytes tentando rodar em CPU.")
        print("====================================================\n")
